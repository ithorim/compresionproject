{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate byte-entropy of a given binary file:\n",
    "\n",
    "Byte entropy is a measure of the average amount of information contained in each byte of a file. It helps us understand how compressible the file is - lower entropy generally means more compressible data.\n",
    "\n",
    "The process involves these steps:\n",
    "1. Read the binary file\n",
    "2. Count occurrences of each byte (0-255)\n",
    "3. Calculate probabilities (pi = Ni / N)\n",
    "4. Calculate entropy using the formula H(p) = -Σ(pi log2(pi))\n",
    "\n",
    "#### Theory:\n",
    "- Entropy, in information theory, quantifies the average amount of information in a message.\n",
    "- For a set of events with probabilities p1, p2, ..., pn, the entropy H is defined as:\n",
    "  H = -Σ(pi * log2(pi))\n",
    "- In our case, each \"event\" is the occurrence of a specific byte value (0-255).\n",
    "- The unit of entropy is bits when using log2.\n",
    "\n",
    "#### Implementation details:\n",
    "\n",
    "1. **File Reading**: \n",
    "   We open the file in binary mode ('rb') to read raw bytes without any encoding.\n",
    "\n",
    "2. **Byte Counting**:\n",
    "   We use a list comprehension with the `count()` method to efficiently count occurrences of each byte value (0-255).\n",
    "\n",
    "3. **Probability Calculation**:\n",
    "   We divide each byte count by the total number of bytes to get probabilities.\n",
    "\n",
    "4. **Entropy Calculation**:\n",
    "   - We use a list comprehension with a conditional statement to handle the case where p = 0 (as log(0) is undefined).\n",
    "   - The `math.log2()` function is used for logarithm calculation.\n",
    "   - We use the `sum()` function to efficiently sum up all the terms.\n",
    "\n",
    "5. **Output**:\n",
    "   We return both the calculated entropy and the file content for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Import the math module for logarithm calculations\n",
    "\n",
    "def calculate_byte_entropy(file_path):\n",
    "    # Open the file in binary read mode\n",
    "    with open(file_path, 'rb') as file:\n",
    "        content = file.read()  # Read the entire file content\n",
    "    \n",
    "    N = len(content)  # Get the total number of bytes in the file\n",
    "    \n",
    "    # Count occurrences of each byte (0-255) in the file\n",
    "    byte_counts = [content.count(i) for i in range(256)]\n",
    "    \n",
    "    # Calculate probability for each byte\n",
    "    probabilities = [count / N for count in byte_counts]\n",
    "    \n",
    "    # Calculate entropy using the formula: H(p) = -Σ(pi * log2(pi))\n",
    "    # We use a list comprehension and sum() for efficiency\n",
    "    # The condition 'if p != 0 else 0' is to handle cases where p = 0 (log(0) is undefined)\n",
    "    entropy = -sum(p * math.log2(p) if p != 0 else 0 for p in probabilities)\n",
    "    \n",
    "    return entropy, content  # Return both the calculated entropy and the file content\n",
    "\n",
    "# Test the function\n",
    "file_path = 'input_shannon_fano_huffman.bin'  # Path to the input file\n",
    "entropy, content = calculate_byte_entropy(file_path)  # Call the function\n",
    "print(f\"Byte entropy: {entropy:.2f}\")  # Print the calculated entropy, rounded to 2 decimal places\n",
    "\n",
    "# Calculate byte counts\n",
    "byte_counts = [content.count(i) for i in range(256)]\n",
    "\n",
    "# Calculate probabilities\n",
    "N = len(content)\n",
    "probabilities = [count / N for count in byte_counts]\n",
    "\n",
    "# Print byte occurrences and probabilities\n",
    "for byte, (count, prob) in enumerate(zip(byte_counts, probabilities)):\n",
    "    if count > 0:  # Only print for bytes that actually appear in the file\n",
    "        char = chr(byte)  # Convert byte to its character representation\n",
    "        print(f\"{char}: Appears {count} times, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysing test results:\n",
    "- This is the calculated entropy of the file, measured in bits per byte.\n",
    "- A value of 2.20 suggests that the file has some redundancy and is potentially compressible.\n",
    "- The maximum entropy for a byte (8 bits) would be 8, which would occur if all 256 possible byte values were equally likely.\n",
    "- Our result of 2.20 indicates that the file's content is not uniformly distributed, which is good for compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Shannon-Fano and Huffman code:\n",
    "#### 2.1 Shannon-Fano:\n",
    "\n",
    "Shannon-Fano coding is a technique for constructing a prefix code based on a set of symbols and their probabilities. It's a foundational algorithm in information theory and data compression.\n",
    "\n",
    "The process involves these steps:\n",
    "1) Create a Symbol class to represent each unique byte and its probability\n",
    "2) Sort the symbols by probability in descending order\n",
    "3) Implement the Shannon-Fano algorithm to assign codes\n",
    "4) Encode the input file using the generated codes\n",
    "5) Write the encoded data to a file\n",
    "\n",
    "#### Theory:\n",
    "- Shannon-Fano coding aims to create shorter codes for more frequent symbols and longer codes for less frequent ones.\n",
    "- It's a \"top-down\" approach, recursively dividing the set of symbols into two subsets.\n",
    "- The algorithm ensures that no codeword is a prefix of another, allowing for unambiguous decoding.\n",
    "- While not optimal like Huffman coding, it often provides good compression ratios.\n",
    "\n",
    "#### Implementation details:\n",
    "\n",
    "1. **Symbol Class**:\n",
    "   - We define a `Symbol` class to hold each byte value, its probability, and its assigned code.\n",
    "   - This object-oriented approach makes it easier to manage and manipulate symbol data.\n",
    "\n",
    "2. **Shannon-Fano Algorithm**:\n",
    "   - The `shannon_fano_coding` function implements the core algorithm:\n",
    "     - It recursively divides the list of symbols into two groups.\n",
    "     - The split point is chosen to minimize the difference in total probability between the groups.\n",
    "     - '0' is assigned to symbols in the first group, '1' to the second.\n",
    "     - This process continues until each group contains only one symbol.\n",
    "\n",
    "3. **Encoding Process**:\n",
    "   - The `encode_file` function applies the generated codes to compress the input file:\n",
    "     - It first writes the symbol table to the output file for later decoding.\n",
    "     - It then reads the input file byte by byte, replacing each byte with its code.\n",
    "     - The encoded data is written to the output file in 8-bit chunks for efficiency.\n",
    "\n",
    "4. **Decoding Process**:\n",
    "   - The `decode_file` function reverses the encoding:\n",
    "     - It reads the symbol table from the encoded file.\n",
    "     - It then reads the encoded data bit by bit, translating code sequences back to original bytes.\n",
    "\n",
    "5. **File Handling**:\n",
    "   - We use context managers (`with` statements) for safe and efficient file operations.\n",
    "   - The encoded file structure includes:\n",
    "     a) The symbol table for decoding\n",
    "     b) The encoded data\n",
    "     c) The total number of encoded bits (to handle padding in the last byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    # For file and path operations\n",
    "import filecmp  # For comparing files\n",
    "\n",
    "# Class to represent a symbol (byte) with its probability and code\n",
    "class Symbol:\n",
    "    def __init__(self, value, probability):\n",
    "        self.value = value          # The byte value (0-255)\n",
    "        self.probability = probability  # Probability of occurrence\n",
    "        self.code = \"\"              # Will store the assigned code\n",
    "\n",
    "# Recursive function to implement Shannon-Fano coding algorithm\n",
    "def shannon_fano_coding(symbols):\n",
    "    if len(symbols) == 1:\n",
    "        return  # Base case: only one symbol, no need to split\n",
    "\n",
    "    # Calculate total probability of all symbols in the current group\n",
    "    total_probability = sum(symbol.probability for symbol in symbols)\n",
    "    current_sum = 0\n",
    "    differences = []\n",
    "\n",
    "    # Find the split point that minimizes the difference between the two groups\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        current_sum += symbol.probability\n",
    "        remainder = total_probability - current_sum\n",
    "        differences.append(abs(current_sum - remainder))\n",
    "\n",
    "    # Index where the difference is minimum is our split point\n",
    "    split_index = differences.index(min(differences))\n",
    "\n",
    "    # Assign '0' to the first group and '1' to the second group\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        if i <= split_index:\n",
    "            symbol.code += \"0\"\n",
    "        else:\n",
    "            symbol.code += \"1\"\n",
    "\n",
    "    # Recursively apply the algorithm to each group\n",
    "    shannon_fano_coding(symbols[:split_index + 1])\n",
    "    shannon_fano_coding(symbols[split_index + 1:])\n",
    "\n",
    "# Function to encode the file using the generated codes\n",
    "def encode_file(input_file, output_file, symbols):\n",
    "    # Create a dictionary for quick lookup of codes\n",
    "    symbol_dict = {symbol.value: symbol.code for symbol in symbols}\n",
    "    \n",
    "    with open(input_file, 'rb') as infile, open(output_file, 'wb') as outfile:\n",
    "        # Write the symbol table to the output file\n",
    "        for symbol in symbols:\n",
    "            outfile.write(f\"{symbol.value}:{symbol.code},\".encode('utf-8'))\n",
    "        outfile.write(b'\\n')  # End of symbol table marker\n",
    "\n",
    "        encoded = ''  # String to hold bits before writing to file\n",
    "        total_bits = 0  # Counter for total number of bits encoded\n",
    "        while True:\n",
    "            byte = infile.read(1)  # Read one byte from input file\n",
    "            if not byte:\n",
    "                break  # End of file\n",
    "            encoded += symbol_dict[byte[0]]  # Append the code for this byte\n",
    "            total_bits += len(symbol_dict[byte[0]])  # Increment total bits\n",
    "            \n",
    "            # Write complete bytes (8 bits) to the output file\n",
    "            while len(encoded) >= 8:\n",
    "                outfile.write(bytes([int(encoded[:8], 2)]))\n",
    "                encoded = encoded[8:]\n",
    "        \n",
    "        # Write any remaining bits, padding with zeros if necessary\n",
    "        if encoded:\n",
    "            outfile.write(bytes([int(encoded.ljust(8, '0'), 2)]))\n",
    "        \n",
    "        # Write the total number of bits (4 bytes, big endian)\n",
    "        outfile.write(total_bits.to_bytes(4, byteorder='big'))\n",
    "\n",
    "# Function to decode the file using the generated codes\n",
    "def decode_file(input_file, output_file, symbols):\n",
    "    # Create a reverse lookup dictionary\n",
    "    code_dict = {symbol.code: symbol.value for symbol in symbols}\n",
    "    \n",
    "    with open(input_file, 'rb') as infile, open(output_file, 'wb') as outfile:\n",
    "        # Read the symbol table\n",
    "        symbol_table = infile.readline().decode('utf-8').strip().split(',')\n",
    "        symbols = [Symbol(int(s.split(':')[0]), 0) for s in symbol_table if s]\n",
    "        for s in symbols:\n",
    "            s.code = symbol_table[[int(x.split(':')[0]) for x in symbol_table if x].index(s.value)].split(':')[1]\n",
    "        \n",
    "        # Read the encoded data\n",
    "        encoded_data = infile.read()\n",
    "        \n",
    "        # The last 4 bytes contain the total number of bits\n",
    "        total_bits = int.from_bytes(encoded_data[-4:], byteorder='big')\n",
    "        encoded_data = encoded_data[:-4]  # Remove the last 4 bytes\n",
    "        \n",
    "        # Convert to binary string\n",
    "        binary_string = ''.join(format(byte, '08b') for byte in encoded_data)\n",
    "        binary_string = binary_string[:total_bits]  # Trim to the correct number of bits\n",
    "        \n",
    "        # Decode\n",
    "        current_code = ''\n",
    "        for bit in binary_string:\n",
    "            current_code += bit\n",
    "            if current_code in code_dict:\n",
    "                outfile.write(bytes([code_dict[current_code]]))\n",
    "                current_code = ''\n",
    "\n",
    "# Main execution\n",
    "input_file = 'input_shannon_fano_huffman.bin'  # Path to input file\n",
    "output_file = 'encoded_shannon_fano.bin'  # Path to output encoded file\n",
    "decoded_file = 'decoded_shannon_fano.bin'  # Path to output decoded file\n",
    "\n",
    "# Read the entire content of the input file\n",
    "with open(input_file, 'rb') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Count occurrences of each byte value\n",
    "byte_counts = [content.count(i) for i in range(256)]\n",
    "total_bytes = len(content)  # Total number of bytes in the file\n",
    "\n",
    "# Create Symbol objects for each byte that appears in the file\n",
    "symbols = [Symbol(i, count/total_bytes) for i, count in enumerate(byte_counts) if count > 0]\n",
    "\n",
    "# Sort symbols by probability in descending order\n",
    "symbols.sort(key=lambda x: x.probability, reverse=True)\n",
    "\n",
    "# Apply Shannon-Fano coding to generate codes\n",
    "shannon_fano_coding(symbols)\n",
    "\n",
    "# Print the generated codes\n",
    "print(\"Shannon-Fano Codes:\")\n",
    "for symbol in symbols:\n",
    "    print(f\"Symbol {symbol.value}: {symbol.code}\")\n",
    "\n",
    "# Encode the file using the generated codes\n",
    "encode_file(input_file, output_file, symbols)\n",
    "\n",
    "# Decode the file\n",
    "decode_file(output_file, decoded_file, symbols)\n",
    "\n",
    "# Print file size comparison\n",
    "print(f\"Original file size: {os.path.getsize(input_file)} bytes\")\n",
    "print(f\"Compressed file size: {os.path.getsize(output_file)} bytes\")\n",
    "print(f\"Decompressed file size: {os.path.getsize(decoded_file)} bytes\")\n",
    "\n",
    "# Compare the original and decoded files\n",
    "if filecmp.cmp(input_file, decoded_file):\n",
    "    print(\"\\nThe files have the same content.\")\n",
    "else:\n",
    "    print(\"\\nThe files do not have the same content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Huffman:\n",
    "\n",
    "Huffman coding is an optimal prefix coding algorithm used for lossless data compression. It creates variable-length codes for symbols, with shorter codes assigned to more frequent symbols.\n",
    "\n",
    "The process involves these steps:\n",
    "1. Create leaf nodes for each symbol.\n",
    "2. Build the Huffman tree:\n",
    "    - Select two nodes with the lowest frequency.\n",
    "    - Create a new internal node with these two as children.\n",
    "    - Assign the sum of the two frequencies to this new node.\n",
    "    - Repeat until only one node remains (the root).\n",
    "3. Generate Huffman codes by traversing the tree.\n",
    "4. Encode the input file using the generated codes.\n",
    "5. Implement decoding process.\n",
    "\n",
    "#### Theory:\n",
    "- Huffman coding creates an optimal prefix code based on the frequency of symbols.\n",
    "- It guarantees that no code is a prefix of another, ensuring unambiguous decoding.\n",
    "- The algorithm builds a binary tree (Huffman tree) bottom-up, unlike Shannon-Fano's top-down approach.\n",
    "- Huffman coding is proven to produce an optimal code for a given set of frequencies and number of symbols.\n",
    "\n",
    "#### Implementation details:\n",
    "\n",
    "1. **Node Class**:\n",
    "   - We define a `Node` class to represent nodes in the Huffman tree.\n",
    "   - Each node contains a symbol, probability, left and right children, and a code.\n",
    "\n",
    "2. **Building Huffman Tree**:\n",
    "   - The `build_huffman_tree` function constructs the Huffman tree:\n",
    "     - It starts with leaf nodes for each symbol.\n",
    "     - It repeatedly selects the two nodes with lowest probabilities, creates a new parent node, and adds it back to the list.\n",
    "     - This process continues until only one node (the root) remains.\n",
    "\n",
    "3. **Generating Codes**:\n",
    "   - The `print_nodes` function traverses the tree to generate and print Huffman codes.\n",
    "   - It uses recursive depth-first traversal, appending '0' for left branches and '1' for right branches.\n",
    "\n",
    "4. **Encoding Process**:\n",
    "   - The `encode_string` function uses the Huffman tree to encode the input data.\n",
    "   - It replaces each symbol with its corresponding Huffman code.\n",
    "\n",
    "5. **File Handling**:\n",
    "   - We use functions to save the Huffman tree structure to a string and read it back.\n",
    "   - This allows us to store the tree structure in the compressed file for later decoding.\n",
    "\n",
    "6. **Decoding Process**:\n",
    "   - The `decode_huffman` function reads the encoded bit string and translates it back to the original symbols using the Huffman tree.\n",
    "\n",
    "7. **Compression and Decompression**:\n",
    "   - We implement the full process of reading an input file, compressing it, writing to a file, reading the compressed file, and decompressing it.\n",
    "   - We also compare the original and decompressed files to verify correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    # For file and path operations\n",
    "import filecmp  # For comparing files\n",
    "\n",
    "# Class to represent a node in the Huffman tree\n",
    "class Node:\n",
    "    def __init__(self, probability, symbol, left=None, right=None):\n",
    "        self.probability = probability  # Probability of the symbol\n",
    "        self.symbol = symbol            # The symbol (byte value)\n",
    "        self.left = left                # Left child node\n",
    "        self.right = right              # Right child node\n",
    "        self.code = ''                  # Huffman code for this node\n",
    "\n",
    "# Function to build the Huffman tree\n",
    "def build_huffman_tree(characters, probabilities):\n",
    "    # Create initial nodes for each character\n",
    "    nodes = [Node(probabilities[i], characters[i]) for i in range(len(characters))]\n",
    "    \n",
    "    while len(nodes) > 1:\n",
    "        # Sort nodes by probability (ascending)\n",
    "        nodes.sort(key=lambda x: x.probability)\n",
    "        # Take the two nodes with lowest probabilities\n",
    "        left = nodes.pop(0)\n",
    "        right = nodes.pop(0)\n",
    "        # Assign '0' to left edge and '1' to right edge\n",
    "        left.code = 0\n",
    "        right.code = 1\n",
    "        # Create a new internal node with these two as children\n",
    "        new_node = Node(left.probability + right.probability, left.symbol + right.symbol, left, right)\n",
    "        # Add the new node back to the list\n",
    "        nodes.append(new_node)\n",
    "    \n",
    "    # Return the root of the Huffman tree\n",
    "    return nodes[0]\n",
    "\n",
    "# Function to print the Huffman codes for each symbol\n",
    "def print_nodes(node, value=''):\n",
    "    new_value = value + str(node.code)\n",
    "    if node.left:\n",
    "        print_nodes(node.left, new_value)\n",
    "    if node.right:\n",
    "        print_nodes(node.right, new_value)\n",
    "    if not node.left and not node.right:\n",
    "        print(f\"{node.symbol} -> {new_value}\")\n",
    "\n",
    "# Function to encode a string using the Huffman tree\n",
    "def encode_string(string, root):\n",
    "    encoded_string = \"\"\n",
    "    for character in string:\n",
    "        encoded_string += find_code(character, root)\n",
    "    return encoded_string\n",
    "\n",
    "# Function to find the Huffman code for a given symbol\n",
    "def find_code(symbol, node, current_code=\"\"):\n",
    "    if not node:\n",
    "        return None\n",
    "    if node.symbol == symbol:\n",
    "        return current_code\n",
    "    else:\n",
    "        left_code = find_code(symbol, node.left, current_code + \"0\")\n",
    "        right_code = find_code(symbol, node.right, current_code + \"1\")\n",
    "        if left_code:\n",
    "            return left_code\n",
    "        elif right_code:\n",
    "            return right_code\n",
    "\n",
    "# Function to save the Huffman tree structure as a string\n",
    "def save_nodes_to_string(node, value=''):\n",
    "    new_value = value + str(node.code)\n",
    "    node_information = \"\"\n",
    "    if node.left:\n",
    "        node_information += save_nodes_to_string(node.left, new_value)\n",
    "    if node.right:\n",
    "        node_information += save_nodes_to_string(node.right, new_value)\n",
    "    if not node.left and not node.right:\n",
    "        node_information += f\"{node.symbol}:{new_value} \"\n",
    "    return node_information\n",
    "\n",
    "# Class to represent a symbol and its code when reading from file\n",
    "class LoadedSymbol:\n",
    "    def __init__(self, value, code):\n",
    "        self.value = value  # The symbol (byte value)\n",
    "        self.code = code    # The Huffman code for this symbol\n",
    "\n",
    "# Function to read the Huffman tree structure from the encoded file\n",
    "def read_first_line_from_binary_file_huffman(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        first_line_bin = file.readline().decode('utf-8').strip()\n",
    "        symbols_raw = first_line_bin.split()\n",
    "        symbols = []\n",
    "        for symbol_raw in symbols_raw:\n",
    "            value, code = symbol_raw.split(\":\")\n",
    "            symbol = LoadedSymbol(value, code)\n",
    "            symbols.append(symbol)\n",
    "    return symbols\n",
    "\n",
    "# Function to read the encoded data from the file\n",
    "def read_rest_from_binary_file_huffman(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        file.readline()  # Skip the first line (symbol table)\n",
    "        data = file.read()\n",
    "    \n",
    "    # The last 4 bytes contain the total number of bits\n",
    "    total_bits = int.from_bytes(data[-4:], byteorder='big')\n",
    "    data = data[:-4]  # Remove the last 4 bytes\n",
    "    \n",
    "    # Convert to binary string\n",
    "    bit_string = ''.join(format(byte, '08b') for byte in data)\n",
    "    return bit_string[:total_bits]  # Return only the valid bits\n",
    "\n",
    "# Function to decode the Huffman-encoded data\n",
    "def decode_huffman(bit_string, symbols):\n",
    "    decoded_string = bytearray()\n",
    "    code_dict = {symbol.code: int(symbol.value) for symbol in symbols}\n",
    "    current_code = \"\"\n",
    "    for bit in bit_string:\n",
    "        current_code += bit\n",
    "        if current_code in code_dict:\n",
    "            decoded_string.append(code_dict[current_code])\n",
    "            current_code = \"\"\n",
    "    return decoded_string\n",
    "\n",
    "# Main execution\n",
    "input_file = 'input_shannon_fano_huffman.bin'  # Input file path\n",
    "output_file = 'encoded_huffman.bin'  # Encoded file path\n",
    "decoded_file = 'decoded_huffman.bin'  # Decoded file path\n",
    "\n",
    "# Read the entire content of the input file\n",
    "with open(input_file, 'rb') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Count occurrences of each byte value\n",
    "byte_counts = [content.count(i) for i in range(256)]\n",
    "total_bytes = len(content)\n",
    "\n",
    "# Create characters and probabilities lists\n",
    "characters = [i for i, count in enumerate(byte_counts) if count > 0]\n",
    "probabilities = [count/total_bytes for count in byte_counts if count > 0]\n",
    "\n",
    "# Build the Huffman tree and get codewords\n",
    "huffman_tree_root = build_huffman_tree(characters, probabilities)\n",
    "print(\"ENCODING:\\n\")\n",
    "print(\"Huffman codewords:\")\n",
    "print_nodes(huffman_tree_root)\n",
    "\n",
    "# Encode the input alphabet\n",
    "encoded_string = encode_string(content, huffman_tree_root)\n",
    "huffman_length = len(encoded_string)\n",
    "\n",
    "# Convert the encoded string to bytes\n",
    "bytes_array = bytearray()\n",
    "for i in range(0, len(encoded_string), 8):\n",
    "    byte = int(encoded_string[i:i+8].ljust(8, '0'), 2)\n",
    "    bytes_array.append(byte)\n",
    "\n",
    "# Write the encoded data to file\n",
    "with open(output_file, \"wb\") as file:\n",
    "    node_information = save_nodes_to_string(huffman_tree_root)\n",
    "    file.write(node_information.encode(\"utf-8\"))\n",
    "    file.write(b\"\\n\")\n",
    "    file.write(bytes_array)\n",
    "    file.write(huffman_length.to_bytes(4, byteorder='big'))\n",
    "\n",
    "# Read the Huffman tree structure and encoded data from file\n",
    "symbols_from_binary_file = read_first_line_from_binary_file_huffman(output_file)\n",
    "content_to_decode = read_rest_from_binary_file_huffman(output_file)\n",
    "\n",
    "print(\"\\nDECODING:\\n\")\n",
    "for symbol in symbols_from_binary_file:\n",
    "    print(f\"Symbol: {symbol.value}: {symbol.code}\")\n",
    "\n",
    "# Decode the data\n",
    "decoded_string = decode_huffman(content_to_decode, symbols_from_binary_file)\n",
    "\n",
    "# Write the decoded data to file\n",
    "with open(decoded_file, 'wb') as file:\n",
    "    file.write(decoded_string)\n",
    "\n",
    "# Compare the original and decoded files\n",
    "if filecmp.cmp(input_file, decoded_file):\n",
    "    print(\"\\nFiles have the same content.\")\n",
    "else:\n",
    "    print(\"\\nFiles do not have the same content.\")\n",
    "\n",
    "# Print file size comparison\n",
    "print(f\"Original file size: {os.path.getsize(input_file)} bytes\")\n",
    "print(f\"Compressed file size: {os.path.getsize(output_file)} bytes\")\n",
    "print(f\"Decompressed file size: {os.path.getsize(decoded_file)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LZ77:\n",
    "\n",
    "LZ77 is a dictionary-based lossless compression algorithm that uses a sliding window technique. It's part of the LZ (Lempel-Ziv) family of algorithms and forms the basis for many popular compression methods.\n",
    "\n",
    "#### Theory:\n",
    "- LZ77 works by replacing repeated occurrences of data with references to a single copy of that data existing earlier in the input stream.\n",
    "- It maintains a \"sliding window\" divided into two parts:\n",
    "  1. A search buffer (dictionary) containing recently processed data.\n",
    "  2. A look-ahead buffer containing the next portion of data to be encoded.\n",
    "- The algorithm looks for matches between the look-ahead buffer and the search buffer.\n",
    "- It encodes data as triplets: (offset, length, next symbol)\n",
    "  - offset: distance to the start of the match in the search buffer\n",
    "  - length: length of the match\n",
    "  - next symbol: the next symbol after the match\n",
    "\n",
    "#### Implementation details:\n",
    "\n",
    "1. **Compression Function (`lz77_compression`)**:\n",
    "   - Input: The string to be compressed and the window size.\n",
    "   - Output: A list of tuples representing the compressed data.\n",
    "   - Process:\n",
    "     - Iterate through the input string.\n",
    "     - For each position, search for the longest match in the sliding window.\n",
    "     - If a match is found, output (1, offset, length).\n",
    "     - If no match is found, output (0, current_character).\n",
    "\n",
    "2. **Writing to Binary File (`lz77_write_to_binary_file`)**:\n",
    "   - This function writes the compressed data to a binary file.\n",
    "   - It uses a simple encoding scheme:\n",
    "     - '0' byte for unmatched characters\n",
    "     - '1' byte for matches, followed by 2 bytes each for offset and length\n",
    "\n",
    "3. **Reading from Binary File (`lz77_read_from_binary_file`)**:\n",
    "   - This function reads the compressed data from the binary file.\n",
    "   - It reverses the encoding process used in writing.\n",
    "\n",
    "4. **Decompression Function (`lz77_decompression`)**:\n",
    "   - Input: The compressed data (list of tuples).\n",
    "   - Output: The decompressed string.\n",
    "   - Process:\n",
    "     - Iterate through the compressed data.\n",
    "     - For unmatched characters (0, char), append the character.\n",
    "     - For matches (1, offset, length), copy the specified sequence from the already decompressed data.\n",
    "\n",
    "5. **File Handling and Verification**:\n",
    "   - The implementation includes functions to read input files, write compressed data, and verify the decompression by comparing with the original file.\n",
    "\n",
    "#### Key Points:\n",
    "- The window size is a crucial parameter that affects both compression ratio and performance.\n",
    "- Larger window sizes can potentially find more matches but increase search time.\n",
    "- The simple (0/1) marker system in the binary file is efficient but limits the maximum offset and length to 65535 (2^16 - 1).\n",
    "- This implementation balances simplicity and effectiveness, making it easy to understand while still providing good compression for many types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import filecmp\n",
    "\n",
    "# Input file path\n",
    "input_file = 'input_lz77.bin'\n",
    "\n",
    "# Read the entire content of the input file\n",
    "with open(input_file, 'rb') as file:\n",
    "    input_alphabet = file.read()\n",
    "\n",
    "def lz77_compression(input_string, window_size):\n",
    "    \"\"\"\n",
    "    Compress the input string using LZ77 algorithm.\n",
    "    \n",
    "    :param input_string: The string to be compressed\n",
    "    :param window_size: The size of the sliding window\n",
    "    :return: A list of tuples representing the compressed data\n",
    "    \"\"\"\n",
    "    compressed_output = []\n",
    "    i = 0  # Current position in the input string\n",
    "\n",
    "    while i < len(input_string):\n",
    "        max_match_length = 0\n",
    "        max_match_index = 0\n",
    "\n",
    "        # Search for the longest match in the sliding window\n",
    "        for j in range(1, min(window_size + 1, i + 1)):\n",
    "            match_length = 0\n",
    "            # Compare characters from current position with those in the window\n",
    "            while (i + match_length < len(input_string) and\n",
    "                   input_string[i + match_length] == input_string[i - j + match_length]):\n",
    "                match_length += 1\n",
    "            # Update if we found a longer match\n",
    "            if match_length > max_match_length:\n",
    "                max_match_length = match_length\n",
    "                max_match_index = i - j\n",
    "\n",
    "        if max_match_length > 0:\n",
    "            # If there's a match, add tuple (1, offset, length) to output\n",
    "            compressed_output.append((1, i - max_match_index, max_match_length))\n",
    "            i += max_match_length\n",
    "        else:\n",
    "            # If no match, add tuple (0, current_character) to output\n",
    "            compressed_output.append((0, input_string[i]))\n",
    "            i += 1\n",
    "\n",
    "    return compressed_output\n",
    "\n",
    "# Set the window size for LZ77 compression\n",
    "window_size = 58  # Larger window size gives more compression but runs slower\n",
    "# Compress the input alphabet\n",
    "compressed_output = lz77_compression(input_alphabet.decode('utf-8'), window_size)\n",
    "\n",
    "def lz77_write_to_binary_file(compressed_output):\n",
    "    \"\"\"\n",
    "    Write the compressed data to a binary file.\n",
    "    \n",
    "    :param compressed_output: List of tuples representing the compressed data\n",
    "    \"\"\"\n",
    "    output_file = 'encoded_lz77.bin'\n",
    "    with open(output_file, \"wb\") as file:\n",
    "        for element in compressed_output:\n",
    "            if element[0] == 0:\n",
    "                # If no match, write (0, character)\n",
    "                file.write(b'\\x00')  # No match marker\n",
    "                file.write(element[1].encode('utf-8'))\n",
    "            elif element[0] == 1:\n",
    "                # If there's a match, write (1, offset, length)\n",
    "                file.write(b'\\x01')  # Match marker\n",
    "                file.write(element[1].to_bytes(2, byteorder='big'))\n",
    "                file.write(element[2].to_bytes(2, byteorder='big'))\n",
    "\n",
    "# Write the compressed data to a binary file\n",
    "lz77_write_to_binary_file(compressed_output)\n",
    "\n",
    "def lz77_read_from_binary_file():\n",
    "    \"\"\"\n",
    "    Read the compressed data from the binary file.\n",
    "    \n",
    "    :return: List of tuples representing the compressed data\n",
    "    \"\"\"\n",
    "    input_file = 'encoded_lz77.bin'\n",
    "    output = []\n",
    "    with open(input_file, 'rb') as file:\n",
    "        while True:\n",
    "            marker = file.read(1)\n",
    "            if not marker:\n",
    "                break  # End of file\n",
    "            if marker == b'\\x00':\n",
    "                # No match, read character\n",
    "                character = file.read(1).decode('utf-8')\n",
    "                output.append((0, character))\n",
    "            elif marker == b'\\x01':\n",
    "                # Match, read offset and length\n",
    "                offset = int.from_bytes(file.read(2), byteorder='big')\n",
    "                length = int.from_bytes(file.read(2), byteorder='big')\n",
    "                output.append((1, offset, length))\n",
    "    return output\n",
    "\n",
    "# Read the compressed data from the binary file\n",
    "compressed_data = lz77_read_from_binary_file()\n",
    "\n",
    "def lz77_decompression(compressed_data):\n",
    "    \"\"\"\n",
    "    Decompress the LZ77 compressed data.\n",
    "    \n",
    "    :param compressed_data: List of tuples representing the compressed data\n",
    "    :return: Decompressed string\n",
    "    \"\"\"\n",
    "    decompressed_string = \"\"\n",
    "    for item in compressed_data:\n",
    "        if item[0] == 0:\n",
    "            # If it's a single character, append it to the result\n",
    "            decompressed_string += item[1]\n",
    "        elif item[0] == 1:\n",
    "            # If it's a match, copy the specified sequence from the already decompressed data\n",
    "            offset, length = item[1], item[2]\n",
    "            start = len(decompressed_string) - offset\n",
    "            for i in range(length):\n",
    "                decompressed_string += decompressed_string[start + i]\n",
    "    return decompressed_string\n",
    "\n",
    "# Decompress the data\n",
    "decompressed_string = lz77_decompression(compressed_data)\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_lz77 = 'input_lz77.bin'\n",
    "decoded_file_lz77 = 'decoded_lz77.bin'\n",
    "\n",
    "# Write the decompressed data to a file\n",
    "with open(decoded_file_lz77, 'wb') as file:\n",
    "    file.write(decompressed_string.encode('utf-8'))\n",
    "\n",
    "# Compare the original and decompressed files\n",
    "if filecmp.cmp(input_file_lz77, decoded_file_lz77):\n",
    "    print(\"\\nFiles have the same content.\")\n",
    "else:\n",
    "    print(\"\\nFiles do not have the same content.\")\n",
    "\n",
    "# Print file size comparison\n",
    "print(f\"Original file size: {os.path.getsize(input_file_lz77)} bytes\")\n",
    "print(f\"Compressed file size: {os.path.getsize('encoded_lz77.bin')} bytes\")\n",
    "print(f\"Decompressed file size: {os.path.getsize(decoded_file_lz77)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LZW (Lempel-Ziv-Welch):\n",
    "\n",
    "LZW is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It's widely used in various file formats and compression utilities due to its simplicity and effectiveness.\n",
    "\n",
    "#### Theory:\n",
    "- LZW is a dictionary-based algorithm that builds its dictionary dynamically as it processes the input.\n",
    "- It starts with a dictionary containing all possible single-character strings.\n",
    "- As it reads input, it adds new, longer strings to the dictionary.\n",
    "- It replaces strings of characters with single codes.\n",
    "- The algorithm doesn't need to transmit the dictionary with the compressed data, as it can be reconstructed during decompression.\n",
    "\n",
    "#### Implementation details:\n",
    "\n",
    "1. **Encoder Function (`lzw_encoder`)**:\n",
    "   - Input: The string to be compressed.\n",
    "   - Output: A list of indices (compressed data) and the dictionary.\n",
    "   - Process:\n",
    "     - Initialize the dictionary with single characters.\n",
    "     - Iterate through the input string:\n",
    "       - If a sequence is in the dictionary, extend it.\n",
    "       - If not, output the code for the current sequence, add the new sequence to the dictionary, and reset.\n",
    "\n",
    "2. **Writing to Binary File**:\n",
    "   - We write the compressed data to a binary file in two parts:\n",
    "     - First line: The unique symbols (initial dictionary)\n",
    "     - Second line: The compressed data (list of indices)\n",
    "\n",
    "3. **Reading from Binary File**:\n",
    "   - We read the compressed data from the binary file:\n",
    "     - First line: Reconstruct the initial dictionary\n",
    "     - Second line: Get the list of indices for decompression\n",
    "\n",
    "4. **Decoder Function (`lzw_decoder`)**:\n",
    "   - Input: Initial symbols and the sequence of indices.\n",
    "   - Output: The decompressed string.\n",
    "   - Process:\n",
    "     - Initialize the dictionary with initial symbols.\n",
    "     - Iterate through the indices:\n",
    "       - Translate each index back to its corresponding string.\n",
    "       - Add new sequences to the dictionary as they're encountered.\n",
    "\n",
    "5. **File Handling and Verification**:\n",
    "   - We implement functions to read input files, write compressed data, read compressed data, and decompress.\n",
    "   - We compare the original and decompressed files to verify correctness.\n",
    "\n",
    "#### Key Points:\n",
    "- LZW is particularly effective for text and data with repeated patterns.\n",
    "- The algorithm adapts to the input data, building an optimal dictionary for the specific content.\n",
    "- Our implementation uses a simple string-based dictionary for clarity, though more efficient data structures could be used for larger inputs.\n",
    "- The compression ratio depends on the nature of the input data and the size of the dictionary.\n",
    "\n",
    "This LZW implementation demonstrates the core principles of the algorithm while providing practical file I/O operations. It's a good balance between simplicity for understanding and functionality for real-world use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import filecmp\n",
    "\n",
    "# Input file path\n",
    "input_file = 'input_lzw.bin'\n",
    "\n",
    "# Read the entire content of the input file\n",
    "with open(input_file, 'rb') as file:\n",
    "    input_alphabet = file.read()\n",
    "\n",
    "# List to store unique symbols from the input\n",
    "unique_symbols = []\n",
    "\n",
    "def lzw_encoder(input_string):\n",
    "    \"\"\"\n",
    "    Compress the input string using the LZW algorithm.\n",
    "    \n",
    "    :param input_string: The string to be compressed\n",
    "    :return: A tuple containing the compressed output and the dictionary used for compression\n",
    "    \"\"\"\n",
    "    # Populate unique_symbols list\n",
    "    for x in input_string:\n",
    "        if x not in unique_symbols:\n",
    "            unique_symbols.append(x)\n",
    "\n",
    "    # Create initial dictionary with unique symbols\n",
    "    dictionary = {}\n",
    "    for x in input_string:\n",
    "        if x not in dictionary:\n",
    "            dictionary[x] = len(dictionary) + 1  # Start from 1, not 0\n",
    "\n",
    "    compressed_output = []\n",
    "    current_sequence = input_string[0]\n",
    "    \n",
    "    # Main LZW compression loop\n",
    "    for char in input_string[1:]:\n",
    "        next_sequence = current_sequence + char\n",
    "        if next_sequence in dictionary:\n",
    "            # If sequence exists, continue to next character\n",
    "            current_sequence = next_sequence\n",
    "        else:\n",
    "            # Output code for current sequence\n",
    "            compressed_output.append(dictionary[current_sequence])\n",
    "            # Add new sequence to dictionary\n",
    "            dictionary[next_sequence] = len(dictionary) + 1\n",
    "            # Reset current sequence\n",
    "            current_sequence = char\n",
    "\n",
    "    # Output code for the last sequence\n",
    "    compressed_output.append(dictionary[current_sequence])\n",
    "    return compressed_output, dictionary\n",
    "\n",
    "# Compress the input alphabet\n",
    "compressed_output, compressed_dictionary = lzw_encoder(input_alphabet.decode('utf-8'))\n",
    "\n",
    "# Prepare results for file writing\n",
    "result_for_file = ' '.join(str(number) for number in compressed_output)\n",
    "symbols_for_file = ' '.join(str(symbol) for symbol in unique_symbols)\n",
    "\n",
    "# Write compressed data to file\n",
    "with open('encoded_lzw.bin', 'wb') as f:\n",
    "    # Write unique symbols on the first line\n",
    "    f.write(symbols_for_file.encode() + b'\\n')\n",
    "    # Write compressed data on the second line\n",
    "    f.write(result_for_file.encode())\n",
    "\n",
    "# Read compressed data from file\n",
    "with open('encoded_lzw.bin', 'rb') as file:\n",
    "    first_line_bin = file.readline()  # Read unique symbols\n",
    "    second_line_bin = file.readline()  # Read compressed data\n",
    "\n",
    "# Decode binary data to string using utf-8\n",
    "first_line_string = first_line_bin.decode('utf-8')\n",
    "second_line_string = second_line_bin.decode('utf-8')\n",
    "\n",
    "# Split string based on spaces and convert to appropriate types\n",
    "symbols_LZW = [str(x) for x in first_line_string.split()]  # Unique symbols\n",
    "indices_LZW = [int(x) for x in second_line_string.split()]  # Compressed data\n",
    "\n",
    "def lzw_decoder(initial_symbols, index_sequence):\n",
    "    \"\"\"\n",
    "    Decompress the LZW compressed data.\n",
    "    \n",
    "    :param initial_symbols: List of initial symbols in the dictionary\n",
    "    :param index_sequence: List of indices representing the compressed data\n",
    "    :return: Decompressed string\n",
    "    \"\"\"\n",
    "    # Initialize dictionary with initial symbols\n",
    "    dictionary = {i + 1: symbol for i, symbol in enumerate(initial_symbols)}\n",
    "    current_index = len(dictionary) + 1\n",
    "    result = [dictionary[index_sequence[0]]]\n",
    "\n",
    "    current_sequence = dictionary[index_sequence[0]]\n",
    "\n",
    "    # Main LZW decompression loop\n",
    "    for index in index_sequence[1:]:\n",
    "        if index in dictionary:\n",
    "            new_sequence = dictionary[index]\n",
    "        elif index == current_index:\n",
    "            # Special case for repeated sequence\n",
    "            new_sequence = current_sequence + current_sequence[0]\n",
    "        else:\n",
    "            raise ValueError('Invalid compressed data')\n",
    "        \n",
    "        result.append(new_sequence)\n",
    "\n",
    "        # Add new sequence to the dictionary\n",
    "        dictionary[current_index] = current_sequence + new_sequence[0]\n",
    "        current_index += 1\n",
    "\n",
    "        # Set current sequence to new sequence\n",
    "        current_sequence = new_sequence\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "# Decompress the data\n",
    "decompressed_result = lzw_decoder(symbols_LZW, indices_LZW)\n",
    "\n",
    "print(f\"Unique symbols: {unique_symbols}\")\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_LZW = 'input_lzw.bin'\n",
    "decoded_file_LZW = 'decoded_lzw.bin'\n",
    "\n",
    "# Write the decompressed data to a file\n",
    "with open(decoded_file_LZW, 'wb') as file:\n",
    "    file.write(decompressed_result.encode('utf-8'))\n",
    "\n",
    "# Compare the original and decompressed files\n",
    "if filecmp.cmp(input_file_LZW, decoded_file_LZW):\n",
    "    print(\"\\nFiles have the same content.\")\n",
    "else:\n",
    "    print(\"\\nFiles do not have the same content.\")\n",
    "\n",
    "# Print file size comparison\n",
    "print(f\"Original file size: {os.path.getsize(input_file_LZW)} bytes\")\n",
    "print(f\"Compressed file size: {os.path.getsize('encoded_lzw.bin')} bytes\")\n",
    "print(f\"Decompressed file size: {os.path.getsize(decoded_file_LZW)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compresion levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Import os module for file operations\n",
    "\n",
    "# Define input and output file paths for each algorithm\n",
    "input_file_shannon_fano_huffman = 'input_shannon_fano_huffman.bin'\n",
    "output_file_shannon_fano = 'encoded_shannon_fano.bin'\n",
    "output_file_huffman = 'encoded_huffman.bin'\n",
    "output_file_lz77 = 'encoded_lz77.bin'\n",
    "\n",
    "# Get the size of the input file for Shannon-Fano and Huffman\n",
    "input_file_size = os.path.getsize(input_file_shannon_fano_huffman)\n",
    "\n",
    "# Get the size of the Shannon-Fano compressed file\n",
    "shannon_fano_size = os.path.getsize(output_file_shannon_fano)\n",
    "\n",
    "# Get the size of the Huffman compressed file\n",
    "huffman_size = os.path.getsize(output_file_huffman)\n",
    "\n",
    "# Calculate compression ratio for Shannon-Fano\n",
    "# Compression ratio = original size / compressed size\n",
    "compression_ratio_shannon_fano = input_file_size / shannon_fano_size\n",
    "compression_ratio_shannon_fano = round(compression_ratio_shannon_fano, 3)  # Round to 3 decimal places\n",
    "print(f\"Compression ratio for Shannon-Fano algorithm: {compression_ratio_shannon_fano}\")\n",
    "\n",
    "# Calculate compression ratio for Huffman\n",
    "compression_ratio_huffman = input_file_size / huffman_size\n",
    "compression_ratio_huffman = round(compression_ratio_huffman, 3)\n",
    "print(f\"Compression ratio for Huffman algorithm: {compression_ratio_huffman}\")\n",
    "\n",
    "# Define input file path for LZ77\n",
    "input_file_lz77 = 'input_lz77.bin'\n",
    "# Get the size of the input file for LZ77\n",
    "input_file_size_lz77 = os.path.getsize(input_file_lz77)\n",
    "# Get the size of the LZ77 compressed file\n",
    "lz77_size = os.path.getsize(output_file_lz77)\n",
    "# Calculate compression ratio for LZ77\n",
    "compression_ratio_lz77 = input_file_size_lz77 / lz77_size\n",
    "compression_ratio_lz77 = round(compression_ratio_lz77, 3)\n",
    "print(f\"Compression ratio for LZ77 algorithm: {compression_ratio_lz77}\")\n",
    "\n",
    "# Define input and output file paths for LZW\n",
    "input_file_lzw = 'input_lzw.bin'\n",
    "output_file_lzw = 'encoded_lzw.bin'\n",
    "# Get the size of the input file for LZW\n",
    "input_file_size_lzw = os.path.getsize(input_file_lzw)\n",
    "# Get the size of the LZW compressed file\n",
    "lzw_size = os.path.getsize(output_file_lzw)\n",
    "# Calculate compression ratio for LZW\n",
    "compression_ratio_lzw = input_file_size_lzw / lzw_size\n",
    "compression_ratio_lzw = round(compression_ratio_lzw, 3)\n",
    "print(f\"Compression ratio for LZW algorithm: {compression_ratio_lzw}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
